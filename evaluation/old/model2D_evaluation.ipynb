{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 CUDA devices\n",
      "Using Seed: 34897567\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')  # to load from any submodule in the repo\n",
    "\n",
    "from models2D import detector2D, predictor2D\n",
    "from utils import dpcr_utils, dpcr_generator\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.device(\"cuda\"):\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using\", torch.cuda.device_count(), \"CUDA devices\")\n",
    "\n",
    "seed_file = open('../utils/seed.txt', \"r\")\n",
    "seed = int(seed_file.read())\n",
    "seed_file.close()\n",
    "\n",
    "print (\"Using Seed:\", seed)\n",
    "    \n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loaded detector model (30 epochs)\n",
      "> Loaded predictor model (30 epochs)\n"
     ]
    }
   ],
   "source": [
    "detector = detector2D.Model(k = 10, emb_dims = 1024, dropout = 0.5).to(device)\n",
    "predictor = predictor2D.Model(k = 10, emb_dims = 1024, dropout = 0.5).to(device)\n",
    "\n",
    "detector = torch.nn.DataParallel(detector)\n",
    "predictor = torch.nn.DataParallel(predictor)\n",
    "\n",
    "detector_checkpoint = torch.load('../models2D/detector2D_model_e30.t7')\n",
    "detector.load_state_dict(detector_checkpoint['model_state_dict'])\n",
    "print (\"> Loaded detector model (%d epochs)\" % detector_checkpoint['epoch'])\n",
    "\n",
    "predictor_checkpoint = torch.load('../models2D/predictor2D_model_e30.t7')\n",
    "predictor.load_state_dict(predictor_checkpoint['model_state_dict'][-1])\n",
    "print (\"> Loaded predictor model (%d epochs)\" % predictor_checkpoint['epoch'])\n",
    "\n",
    "_ = detector.eval()\n",
    "_ = predictor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time:  0.04598546028137207\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "testArr = dpcr_generator.getTrainingArray(100, resolution = 50, max_iter = 5, gamma = 1.2)\n",
    "\n",
    "print (\"Total Time: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClusterPoints(pts, threshold):\n",
    "    \n",
    "    neighbor_index = sp.spatial.distance.cdist(pts, pts) < threshold\n",
    "    \n",
    "    groups = []\n",
    "    \n",
    "    for i in range(neighbor_index.shape[0]):\n",
    "        \n",
    "        group = None\n",
    "        \n",
    "        # check if point exists in another group\n",
    "        for k in range(len(groups)):\n",
    "            if i in groups[k]:\n",
    "                group = k\n",
    "                break\n",
    "                \n",
    "        if group == None:\n",
    "            \n",
    "            groups.append(np.nonzero(neighbor_index[i, :].reshape(-1))[0].tolist())\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            groups[group] += np.nonzero(neighbor_index[i, :].reshape(-1))[0].tolist()\n",
    "                \n",
    "    \n",
    "    cluster_pts = np.zeros((len(groups), pts.shape[1]))\n",
    "    for i in range(len(groups)):\n",
    "        cluster_pts[i] = np.mean(pts[groups[i]], axis = 0)\n",
    "\n",
    "    return cluster_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testArr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ac9ef42040a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mentry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestArr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestArr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0morigin_points\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpoints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'testArr' is not defined"
     ]
    }
   ],
   "source": [
    "entry = testArr[np.random.randint(0, len(testArr))]\n",
    "\n",
    "origin_points = np.copy(entry[:,0:2])\n",
    "\n",
    "points = entry[:,0:2]\n",
    "nearest_hidden = entry[:,2:4]\n",
    "edge_mask = entry[:,4].astype(bool)\n",
    "\n",
    "extra_points = []\n",
    "\n",
    "iteration_count = 0\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    pts = torch.from_numpy(np.swapaxes(np.expand_dims(points, axis = 0), 1,2)).float().to(device)\n",
    "    \n",
    "    edge_points_prediction = detector(pts).squeeze(0).transpose(0,1)\n",
    "    new_points_prediction = predictor(pts).squeeze(0).transpose(0,1)\n",
    "    \n",
    "    new_points_prediction_np = new_points_prediction.detach().cpu().numpy()\n",
    "    \n",
    "    edge_points_prediction_np = edge_points_prediction.detach().cpu().numpy()\n",
    "    edge_points_prediction_np = np.argmax(edge_points_prediction_np, axis = 1).astype(bool)\n",
    "    \n",
    "    del pts\n",
    "    del edge_points_prediction\n",
    "    del new_points_prediction\n",
    "    \n",
    "    iteration_count += 1\n",
    "    \n",
    "    if not np.any(edge_points_prediction_np):\n",
    "        \n",
    "        break\n",
    "        \n",
    "    else:\n",
    "\n",
    "        new_points = (points + new_points_prediction_np)[edge_points_prediction_np]\n",
    " \n",
    "        dists = sp.spatial.distance.cdist(new_points, points)\n",
    "    \n",
    "        new_points = new_points[np.amin(dists >= 0.05, axis = 1).astype(bool)]\n",
    "        \n",
    "        new_points = getClusterPoints(new_points, 0.05)\n",
    "        \n",
    "        if new_points.shape[0] == 0:\n",
    "            \n",
    "            break\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            extra_points.append(new_points)\n",
    "            points = np.concatenate((points, new_points), axis = 0)\n",
    "        \n",
    "\n",
    "print (\"Ran %d iterations!\" % (iteration_count))\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(16,9))\n",
    "\n",
    "plt.axis('equal')\n",
    "\n",
    "ax.set_facecolor([0.5,0.5,0.5])\n",
    "ax.grid(True)\n",
    "ax.axis('equal')\n",
    "\n",
    "ax.scatter(\n",
    "    origin_points[:,0],\n",
    "    origin_points[:,1],\n",
    "    color = 'black',\n",
    "    label = 'input',\n",
    "    s = 100\n",
    ")\n",
    "\n",
    "for i in range(len(extra_points)):\n",
    "    \n",
    "    fac = (i+1) / len(extra_points)\n",
    "    \n",
    "    ax.scatter(\n",
    "        extra_points[i][:,0],\n",
    "        extra_points[i][:,1],\n",
    "        alpha = 0.9,\n",
    "        label = \"iteration %d\" % (i+1),\n",
    "        color = matplotlib.cm.get_cmap('viridis')(fac),\n",
    "        s = 100\n",
    "    )\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "#plt.savefig(\"example_.png\", dpi = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
