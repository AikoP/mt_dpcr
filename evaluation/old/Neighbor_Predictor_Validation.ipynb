{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 CUDA devices\n",
      "Using Seed: 34897567\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')  # to load from any submodule in the repo\n",
    "\n",
    "import utils.readOBJ as reader\n",
    "import utils.dpcr_utils as utils\n",
    "import utils.dpcr_generator as generator\n",
    "\n",
    "import models3D.neighbor_predictor3D as neighbor_predictor3D\n",
    "import models3D.detector3D as detector3D\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import itertools\n",
    "import _pickle as cPickle\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.device(\"cuda\"):\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using\", torch.cuda.device_count(), \"CUDA devices\")\n",
    "    \n",
    "torch.no_grad()\n",
    "\n",
    "seed_file = open('../utils/seed.txt', \"r\")\n",
    "seed = int(seed_file.read())\n",
    "seed_file.close()\n",
    "\n",
    "print (\"Using Seed:\", seed)\n",
    "    \n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loaded predictor model (50 epochs)\n"
     ]
    }
   ],
   "source": [
    "predictor = neighbor_predictor3D.Model(k = 20, emb_dims = 1024, dropout = 0.5, neighbors = 6).to(device)\n",
    "predictor = torch.nn.DataParallel(predictor)\n",
    "\n",
    "#detector = detector3D.Model(k = 10, emb_dims = 1024, dropout = 0.5).to(device)\n",
    "#detector = torch.nn.DataParallel(detector)\n",
    "\n",
    "predictor_checkpoint = torch.load('../models3D/model_predictor_3d_batched_0624_e50.t7')\n",
    "#detector_checkpoint = torch.load('../models3D/model_detector_3d_batched_e50.t7')\n",
    "\n",
    "predictor.load_state_dict(predictor_checkpoint['model_state_dict'][-1])\n",
    "print (\"> Loaded predictor model (%d epochs)\" % len(predictor_checkpoint['train_time']))\n",
    "\n",
    "#detector.load_state_dict(detector_checkpoint['model_state_dict'])\n",
    "#print (\"> Loaded detector model (%d epochs)\" % detector_checkpoint['epoch'])\n",
    "\n",
    "_ = predictor.eval()\n",
    "#_ = detector.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = None\n",
    "with open('../data/train_test_data_new/train_data_test', 'rb') as file:\n",
    "    train_data = cPickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_count = 6\n",
    "\n",
    "# extract train- & test data (and move to device) --------------------------------------------\n",
    "\n",
    "pts = train_data[\"pts\"].float()\n",
    "knn = train_data[\"knn\"]\n",
    "\n",
    "#train_bins = train_data[\"train_bins\"]\n",
    "test_samples = train_data[\"test_samples\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 157 Test Models\n",
      "Total Time: 5.144001722335815\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "model, _ = reader.readOBJ(\"../data/models/faces/clean/face_%02d_clean.obj\" % (12))\n",
    "    \n",
    "model = torch.from_numpy(model).to(device)\n",
    "\n",
    "# mean centering\n",
    "model = model - torch.mean(model, dim = 0)\n",
    "\n",
    "# scaling to fit 2x2x2 bounding box\n",
    "min_vals, _ = torch.min(model, dim = 0)\n",
    "max_vals, _ = torch.max(model, dim = 0)\n",
    "scale = torch.max(torch.abs(min_vals),  torch.abs(max_vals))\n",
    "model = model / scale\n",
    "\n",
    "knn = utils.knn(model, 6)\n",
    "pts = model.float()\n",
    "\n",
    "test_samples = generator.getData(pts, 20)\n",
    "\n",
    "print (\"Generated %d Test Models\" % len(test_samples))\n",
    "print (\"Total Time:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model 11/10..\n",
      "   ..done! (3.4s)\n",
      "Processing model 12/10..\n",
      "   ..done! (3.3s)\n",
      "\n",
      "#Test Samples: 139\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "test_samples = []\n",
    "knn = []\n",
    "\n",
    "start_id = 0\n",
    "\n",
    "for i in range(11,13):\n",
    "    \n",
    "    print (\"Processing model %d/%d..\" % (i, 10))\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    model, _ = reader.readOBJ(\"../data/models/faces/clean/face_%02d_clean.obj\" % (i))\n",
    "    \n",
    "    model = torch.from_numpy(model).to(device)\n",
    "    \n",
    "    # mean centering\n",
    "    model = model - torch.mean(model, dim = 0)\n",
    "    \n",
    "    # scaling to fit 2x2x2 bounding box\n",
    "    min_vals, _ = torch.min(model, dim = 0)\n",
    "    max_vals, _ = torch.max(model, dim = 0)\n",
    "    scale = torch.max(torch.abs(min_vals),  torch.abs(max_vals))\n",
    "    model = model / scale\n",
    "    \n",
    "    models.append(model)\n",
    "    \n",
    "    knn.append(utils.knn(model, 6) + start_id)\n",
    "    \n",
    "    for sample in generator.getData(model, 10):\n",
    "    \n",
    "        sample[:, 0] += start_id\n",
    "        test_samples.append(sample)\n",
    "    \n",
    "    print (\"   ..done! (%.1fs)\" % (time.time() - start))\n",
    "    \n",
    "    start_id += model.size(0)\n",
    "    \n",
    "    \n",
    "print (\"\\n#Test Samples: %d\" % len(test_samples))\n",
    "\n",
    "pts = torch.cat(models).float()\n",
    "knn = torch.cat(knn).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pre-Computing Neighbor Permutations..\n",
      "  > done! (0.95s)\n"
     ]
    }
   ],
   "source": [
    "neighbors_count = 6\n",
    "\n",
    "# pre-permute the neighbors (labels) ----------------------------------------------------------\n",
    "\n",
    "print (\"\\nPre-Computing Neighbor Permutations..\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "neighbors_dirs = pts[knn] - pts[:, None, :]\n",
    "\n",
    "perms_np = np.array(list(itertools.permutations(np.arange(neighbors_count))))\n",
    "perms = torch.from_numpy(perms_np).long().to(device)\n",
    "\n",
    "neighbors_dirs_perms = torch.zeros((neighbors_dirs.size(0), perms.size(0), neighbors_count, 3))\n",
    "\n",
    "for (i, p) in enumerate(perms):\n",
    "    neighbors_dirs_perms[:,i] = neighbors_dirs[:, p, :]\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print (\"  > done! (%.2fs)\" % (end - start))\n",
    "\n",
    "\n",
    "# set up loss and batch size -------------------------------------------------------------------\n",
    "\n",
    "loss_function = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "> Loaded predictor model (1 epochs)\n",
      "\n",
      "Epoch 1:\n",
      "Average Validation Loss: 16.553\n",
      "Average Validation Loss (per point): 0.003\n",
      "> Loaded predictor model (2 epochs)\n",
      "\n",
      "Epoch 2:\n",
      "Average Validation Loss: 16.553\n",
      "Average Validation Loss (per point): 0.003\n",
      "> Loaded predictor model (3 epochs)\n",
      "\n",
      "Epoch 3:\n",
      "Average Validation Loss: 16.553\n",
      "Average Validation Loss (per point): 0.003\n",
      "> Loaded predictor model (4 epochs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-b45743f9d598>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                 \u001b[1;31m# size: (N, NC, 3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mt_perm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneighbors_dirs_perms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m                 \u001b[1;31m# size: (N, perm_count, NC, 3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# for each vertex compute a permutation of neighbors s.t. the sum of differences to the prediction is minimized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "validation_loss = len(predictor_checkpoint['model_state_dict']) * [0]\n",
    "\n",
    "print (validation_loss)\n",
    "\n",
    "for i in range(len(predictor_checkpoint['model_state_dict'])):\n",
    "    \n",
    "    predictor = neighbor_predictor3D.Model(k = 20, emb_dims = 1024, dropout = 0.5, neighbors = 6).to(device)\n",
    "    predictor = torch.nn.DataParallel(predictor)\n",
    "\n",
    "    predictor.load_state_dict(predictor_checkpoint['model_state_dict'][i])\n",
    "    print (\"> Loaded predictor model (%d epochs)\" % (i+1))\n",
    "\n",
    "    _ = predictor.eval()\n",
    "\n",
    "    cum_test_loss = 0\n",
    "    cum_test_loss_per_point = 0\n",
    "    loss_function = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "    for sample in test_samples:\n",
    "\n",
    "        sample_ids = sample[:,0].flatten()\n",
    "\n",
    "        input_tensor = pts[sample_ids].unsqueeze(0).transpose(1,2).to(device)\n",
    "\n",
    "        p = predictor(input_tensor)  # size: (1, 3*NC, N)\n",
    "        p = p.squeeze(0)                                        # size: (3*NC, N)\n",
    "        p = p.transpose(0,1)                                    # size: (N, 3*NC)\n",
    "        p = p.reshape((-1, 6, 3))                 # size: (N, NC, 3)\n",
    "\n",
    "        t_perm = neighbors_dirs_perms[sample_ids].to(device)                 # size: (N, perm_count, NC, 3)\n",
    "\n",
    "        # for each vertex compute a permutation of neighbors s.t. the sum of differences to the prediction is minimized\n",
    "        minmatches = utils.matchPoints(p.detach(), y=None, perms=perms, y_perms=t_perm)  # size: (N, NC)\n",
    "\n",
    "        # expand the permutation index\n",
    "        gather_idx = minmatches.expand(3,minmatches.size(0),minmatches.size(1)).transpose(0,1).transpose(1,2)  # size: (N, NC, 3)\n",
    "\n",
    "        # gather neighbor directions according to the permutations\n",
    "        t = neighbors_dirs[sample_ids].gather(1, gather_idx).to(device)    # size: (N, NC, 3)\n",
    "\n",
    "        loss = loss_function(p, t).item()\n",
    "        loss_per_point = loss / sample_ids.numel()\n",
    "\n",
    "        #print (\"\\nsample loss:\", loss)\n",
    "        #print (\"sample loss_per_point:\", loss_per_point)\n",
    "\n",
    "        cum_test_loss += loss\n",
    "        cum_test_loss_per_point += loss_per_point\n",
    "        \n",
    "    validation_loss[i] = cum_test_loss\n",
    "    \n",
    "    print (\"\\nEpoch %d:\" % (i+1))\n",
    "    print (\"Average Validation Loss: %.3f\" % (cum_test_loss / len(test_samples)))\n",
    "    print (\"Average Validation Loss (per point): %.3f\" % (cum_test_loss_per_point / len(test_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "\n",
    "import copy\n",
    "\n",
    "b = copy.deepcopy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.2 64-bit",
   "language": "python",
   "name": "python36264bit0676f2f61ede4885a79bdf760204de6d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
